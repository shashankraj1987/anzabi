{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import log_config as lc\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_loc = \"/home/shashankraj/Documents/DATA/\"\n",
    "\n",
    "data_loc = r'D:\\Offline_Docs\\Anza\\DATA_Dump'\n",
    "log_file = data_loc+\"\\\\\"+\"Logs\"\n",
    "final_files = data_loc+\"\\\\\"+\"Final_Df\"\n",
    "processed = data_loc+\"\\\\\"+\"Processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_rows = {'Client Billing Descending': 0, \"Fee Breakdown by Dept and Fee Earner\": 3,\n",
    "             \"Fee Summary by Dept and Fee Earner\": 3, \"Fees Billed\": 3, \"Matter Source of Business inc Matter Bills\": 0,\n",
    "             \"Matters Opened by FE\": 3, \"Payment Received Analysis\": 3, \"Total Hours by Fee Earner-With Billings\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df):\n",
    "    \"\"\" \n",
    "        This will remove all the columns that contain the word Textbox in them. \n",
    "        This Function takes a DataFrame as in input and returns all the columns except TextBox. \n",
    "    \"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "    txt_chk = re.compile(r'Textbox')\n",
    "    tot_hrs_col_name = [\"RecordedHours2\",\"NonChargeHours2\",\"WOHours2\",\"TotalHour2\", \"bankRef\"]\n",
    "    new_cols = [col_name for col_name in cols if not(txt_chk.search(col_name)) and col_name not in tot_hrs_col_name]\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(dct, match):\n",
    "    \"\"\"\n",
    "        Takes a Dictionary and a filename as inputs and Returns how many rows need to be skipped for a filename. \n",
    "        Returns the Number of rows to skip, while creating a DataFrame.\n",
    "    \"\"\"\n",
    "    for val in dct.keys():\n",
    "        if re.match(val, match):\n",
    "            return dct[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_Filename(fname):\n",
    "    \"\"\"\n",
    "        Accepts a Filename that has fname_date.csv format. \n",
    "        It Extracts the From Date form the File and Returns the same. \n",
    "        These Are Datetime Objects.  \n",
    "        If the Filename has only start date, it will just return the same date for Both Start and End Date. \n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.compile(r'_\\d*')\n",
    "    match = pattern.findall(fname)\n",
    "    dt = match[0]\n",
    "    dt = dt.split(\"_\")[1]\n",
    "    file_date = pd.to_datetime(dt, format='%d%m%Y')\n",
    "    return file_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files(file_loc):\n",
    "    log_loc = file_loc + \"\\\\\" + \"Logs\"\n",
    "    cat_file_logger = lc.start_log(log_loc)\n",
    "    unprocessed = file_loc + \"\\\\Unprocessed\"\n",
    "    processed = file_loc + \"\\\\\" + \"Processed\"\n",
    "\n",
    "    if os.name == 'posix':\n",
    "        os.system('clear')\n",
    "    else:\n",
    "        os.system('cls')\n",
    "\n",
    "    process_files = {}\n",
    "    discard_files = {}\n",
    "    file_list = os.listdir(file_loc)\n",
    "\n",
    "    if os.path.exists(unprocessed):\n",
    "        cat_file_logger.info(f'\\n[Unprocessed] already Exists in [{file_loc}]\\n')\n",
    "    else:\n",
    "        os.mkdir(unprocessed)\n",
    "        cat_file_logger.info(f'\\nCreating Folder {unprocessed} in {file_loc}\\n')\n",
    "\n",
    "    # Segregating the Files.\n",
    "    discard_files['all_pie'] = [files for files in file_list if len(re.compile(r'[\\sa-zA-Z\\s]+Pie \\w+_\\d+.csv').findall(files))]\n",
    "    discard_files['all_xlsx'] = [files for files in file_list if files.endswith(\".xlsx\")]\n",
    "\n",
    "    # Move the above files to Unprocessed Folder before moving ahead\n",
    "    for f in discard_files.keys():\n",
    "        [shutil.move(file_loc + \"\\\\\" + file, unprocessed)  for file in discard_files[f]]\n",
    "\n",
    "    process_files['client_billing'] = [files for files in file_list if len(re.compile(r'Client [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['fee_brkdn_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Breakdown [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['fee_summ_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Summary [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['fees_billed'] = [files for files in file_list if len(re.compile(r'Fees B[a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['matter_src'] = [files for files in file_list if len(re.compile(r'Matter Source [a-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    process_files['matter_opened'] = [files for files in file_list if len(re.compile(r'Matters Open[\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    process_files['payment_rcv'] = [files for files in file_list if len(re.compile(r'Payment [\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    process_files['tot_hrs_fe'] = [files for files in file_list if len(re.compile(r'([tT]otal[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "\n",
    "    for f in process_files.keys():\n",
    "        print(f'Moving category [{f}]')\n",
    "        [shutil.move(file_loc + \"\\\\\" + file, processed)  for file in process_files[f]]\n",
    "    \n",
    "\n",
    "    return process_files\n",
    "    #:TODO: \n",
    "    # Add these files individually to a database.\n",
    "    # Then concatenate these files and add them to staging database. \n",
    "    # No Need to move files in separate folders. Create a Dataframe in memory and perform operations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files2(file_loc):\n",
    "\n",
    "    import sys\n",
    "    \n",
    "    if os.name == 'posix':\n",
    "            os.system('clear')\n",
    "    else:\n",
    "        os.system('cls')\n",
    "\n",
    "    log_loc = file_loc + \"/Logs\"\n",
    "    cat_file_logger = lc.start_log(log_loc)\n",
    "    unprocessed = file_loc + \"/Unprocessed\"\n",
    "    processed = file_loc + \"/Processed\"\n",
    "\n",
    "    ess_fold_list = [log_loc,unprocessed,processed]\n",
    "\n",
    "    for fold in ess_fold_list:\n",
    "        if os.path.exists(fold):\n",
    "            cat_file_logger.info(f'\\n[{fold}] already Exists in [{file_loc}]\\n')\n",
    "        else:\n",
    "            os.mkdir(file_loc+\"/\"+fold)\n",
    "            cat_file_logger.info(f'\\nCreating Folder {fold} in {file_loc}\\n')\n",
    "\n",
    "    total_csv = len([f for f in os.listdir(file_loc) if f.endswith('.csv')])\n",
    "    if total_csv > 0:\n",
    "        print(f\"Found {total_csv} csv files\")\n",
    "    elif total_csv == 0:\n",
    "        print(\"No CSV Files Found. Exiting.\")\n",
    "        sys.exit()\n",
    "\n",
    "    process_files = {}\n",
    "    discard_files = {}\n",
    "    file_list = os.listdir(file_loc)\n",
    "\n",
    "    cat_file_logger.info(f\"Any xls file and files having Pie in the names will not be Processed\")\n",
    "\n",
    "    # Segregating the Files.\n",
    "    discard_files['all_pie'] = [files for files in file_list if len(re.compile(r'[\\sa-zA-Z\\s]+Pie \\w+_\\d+.csv').findall(files))]\n",
    "    discard_files['all_xlsx'] = [files for files in file_list if files.endswith(\".xlsx\")]\n",
    "\n",
    "    # Move the above files to Unprocessed Folder before moving ahead\n",
    "    for f in discard_files.keys():\n",
    "                cat_file_logger.info(f'Moving out files of list [{f}] to folder [{unprocessed}]')\n",
    "                # [shutil.move(file_loc + \"/\" + file, unprocessed)  for file in discard_files[f]]\n",
    "                [cat_file_logger.info(f\"Moving File {file} to  unprocessed\")  for file in discard_files[f]]\n",
    "\n",
    "    process_files['client_billing'] = [files for files in file_list if len(re.compile(r'Client [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['client_billing'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Client BIlling ')\n",
    "    \n",
    "    process_files['fee_brkdn_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Breakdown [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fee_brkdn_dept_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fee Breakdown by Dept ')\n",
    "    \n",
    "    process_files['fee_summ_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Summary [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fee_summ_dept_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fee Summary by Dept ')\n",
    "        \n",
    "    process_files['fees_billed'] = [files for files in file_list if len(re.compile(r'Fees B[a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['fees_billed'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Fees BIlled ')\n",
    "        \n",
    "    process_files['matter_src'] = [files for files in file_list if len(re.compile(r'Matter Source [a-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['matter_src'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Matter Source Reference ')\n",
    "     \n",
    "    process_files['matter_opened'] = [files for files in file_list if len(re.compile(r'Matters Open[\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['matter_opened'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Matter Opened by FE ')\n",
    "    \n",
    "    process_files['payment_rcv'] = [files for files in file_list if len(re.compile(r'Payment [\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    cnt = len(process_files['payment_rcv'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Payment Received ')\n",
    "       \n",
    "    process_files['tot_hrs_fe'] = [files for files in file_list if len(re.compile(r'([tT]otal[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "    cnt = len(process_files['tot_hrs_fe'])\n",
    "    cat_file_logger.info(f'Found [{cnt}] files of Total Hours by Fee Earner ')\n",
    "\n",
    "    for f in process_files.keys():\n",
    "        cat_file_logger.info(f'Moving category [{f}]')\n",
    "        try:\n",
    "            #[shutil.move(file_loc + \"/\" + file, processed)  for file in process_files[f]]\n",
    "            [cat_file_logger.info(f' Moving files {file} to processed')  for file in process_files[f]]\n",
    "        except:\n",
    "            cat_file_logger.info(\"Error Moving File {}\".format(f))\n",
    "        else:\n",
    "            #[cat_file_logger.info(f'Moving File --> {file}')  for file in process_files[f]]\n",
    "            pass\n",
    "        \n",
    "    return process_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106 csv files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'client_billing': ['Client Billing Descending_01032022.csv',\n",
       "  'Client Billing Descending_02032022.csv',\n",
       "  'Client Billing Descending_03032022.csv',\n",
       "  'Client Billing Descending_04032022.csv',\n",
       "  'Client Billing Descending_07032022.csv',\n",
       "  'Client Billing Descending_08032022.csv',\n",
       "  'Client Billing Descending_09032022.csv',\n",
       "  'Client Billing Descending_10032022.csv',\n",
       "  'Client Billing Descending_11032022.csv',\n",
       "  'Client Billing Descending_14032022.csv',\n",
       "  'Client Billing Descending_15032022.csv',\n",
       "  'Client Billing Descending_16032022.csv'],\n",
       " 'fee_brkdn_dept_fe': ['Fee Breakdown by Dept and Fee Earner_01032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_02032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_03032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_04032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_07032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_08032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_09032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_10032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_11032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_14032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_15032022.csv',\n",
       "  'Fee Breakdown by Dept and Fee Earner_16032022.csv'],\n",
       " 'fee_summ_dept_fe': ['Fee Summary by Dept and Fee Earner_01032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_02032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_03032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_04032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_07032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_08032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_09032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_10032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_11032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_14032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_15032022.csv',\n",
       "  'Fee Summary by Dept and Fee Earner_16032022.csv'],\n",
       " 'fees_billed': ['Fees Billed_01032022.csv',\n",
       "  'Fees Billed_02032022.csv',\n",
       "  'Fees Billed_03032022.csv',\n",
       "  'Fees Billed_04032022.csv',\n",
       "  'Fees Billed_07032022.csv',\n",
       "  'Fees Billed_08032022.csv',\n",
       "  'Fees Billed_09032022.csv',\n",
       "  'Fees Billed_10032022.csv',\n",
       "  'Fees Billed_11032022.csv',\n",
       "  'Fees Billed_14032022.csv',\n",
       "  'Fees Billed_15032022.csv',\n",
       "  'Fees Billed_16032022.csv'],\n",
       " 'matter_src': ['Matter Source of Business inc Matter Bills (Bill Date)_01032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_02032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_03032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_04032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_07032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_08032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_09032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_10032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_11032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_14032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_15032022.csv',\n",
       "  'Matter Source of Business inc Matter Bills (Bill Date)_16032022.csv'],\n",
       " 'matter_opened': ['Matters Opened by FE_01032022.csv',\n",
       "  'Matters Opened by FE_02032022.csv',\n",
       "  'Matters Opened by FE_03032022.csv',\n",
       "  'Matters Opened by FE_04032022.csv',\n",
       "  'Matters Opened by FE_05032022.csv',\n",
       "  'Matters Opened by FE_07032022.csv',\n",
       "  'Matters Opened by FE_08032022.csv',\n",
       "  'Matters Opened by FE_09032022.csv',\n",
       "  'Matters Opened by FE_10032022.csv',\n",
       "  'Matters Opened by FE_11032022.csv',\n",
       "  'Matters Opened by FE_12032022.csv',\n",
       "  'Matters Opened by FE_14032022.csv',\n",
       "  'Matters Opened by FE_15032022.csv',\n",
       "  'Matters Opened by FE_16032022.csv'],\n",
       " 'payment_rcv': ['Payment Received Analysis_01032022.csv',\n",
       "  'Payment Received Analysis_02032022.csv',\n",
       "  'Payment Received Analysis_03032022.csv',\n",
       "  'Payment Received Analysis_04032022.csv',\n",
       "  'Payment Received Analysis_07032022.csv',\n",
       "  'Payment Received Analysis_08032022.csv',\n",
       "  'Payment Received Analysis_09032022.csv',\n",
       "  'Payment Received Analysis_10032022.csv',\n",
       "  'Payment Received Analysis_11032022.csv',\n",
       "  'Payment Received Analysis_14032022.csv',\n",
       "  'Payment Received Analysis_15032022.csv',\n",
       "  'Payment Received Analysis_16032022.csv'],\n",
       " 'tot_hrs_fe': ['Total Hours by Fee Earner-With Billings All_01032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_02032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_03032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_04032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_05032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_06032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_07032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_08032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_09032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_10032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_11032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_12032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_13032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_14032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_15032022.csv',\n",
       "  'Total Hours by Fee Earner-With Billings All_16032022.csv']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorize_files2(data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(dict_list, file_loc, logfile_loc):\n",
    "    concat_logger = lc.start_log(logfile_loc)\n",
    "    df_all_files = {}\n",
    "    dict_fname = \"\"\n",
    "     # If a Filename is not in this Dictionary, then it will not be Considered. \n",
    "    # date = (datetime.now()).strftime(\"%m-%d-%y\")\n",
    "    \n",
    "    for file_cat in dict_list.keys():\n",
    "        df_final = pd.DataFrame()\n",
    "        concat_logger.info('*' * 50)\n",
    "        concat_logger.info(f'Processing Category {file_cat}')\n",
    "        for file in dict_list[file_cat]:\n",
    "            dict_fname = file.split(\"_\")[0]\n",
    "            dfc_file = pd.read_csv((file_loc+\"\\\\Processed\\\\\" + file), skiprows=get_rows(skip_rows, file))\n",
    "            dfc_file = dfc_file[remove_cols(dfc_file)]\n",
    "            processing_date = get_date_from_Filename(file)\n",
    "            dfc_file[\"Date_Added\"] = processing_date\n",
    "            df_final = pd.concat([df_final, dfc_file], ignore_index=True)\n",
    "            df_final.fillna(0)\n",
    "            df_final = df_final.replace(re.compile(r'Â£'), \"\").replace(re.compile(r','), \"\").replace(re.compile(r'\\('),\"\").replace(re.compile(r'\\)'), \"\")\n",
    "            print(df_final.head())\n",
    "            for cols in df_final.columns:\n",
    "                try:\n",
    "                    df_final[cols].astype(float)\n",
    "                except:\n",
    "                    continue\n",
    "                    # concat_logger.info(f'Skipping Column {cols}')\n",
    "                else:\n",
    "                    # concat_logger.info(f'Converting {cols} to float')\n",
    "                    df_final[cols] = df_final[cols].astype(float)\n",
    "\n",
    "        df_all_files[dict_fname] = df_final\n",
    "\n",
    "    for f in df_all_files.keys():\n",
    "        rows = df_all_files[f].shape[0]\n",
    "        concat_logger.info(f'Will Add -> {rows} entries for [{f}] to the database')\n",
    "\n",
    "    return df_all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_list = categorize_files(file_loc)\n",
    "# all_files = concat_files(dict_list, file_loc, log_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
