{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import log_config as lc\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_loc = \"/home/shashankraj/Documents/DATA/\"\n",
    "file_loc = r'D:\\One Drive Anza\\OneDrive - Anza Services LLP\\DATA_Dump'\n",
    "unprocessed = file_loc+\"\\\\Unprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_rows = {'Client Billing Descending': 0, \"Fee Breakdown by Dept and Fee Earner\": 3,\n",
    "             \"Fee Summary by Dept and Fee Earner\": 3, \"Fees Billed\": 3, \"Matter Source of Business inc Matter Bills\": 0,\n",
    "             \"Matters Opened by FE\": 3, \"Payment Received Analysis\": 3, \"Total Hours by Fee Earner-With Billings\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df):\n",
    "    \"\"\" \n",
    "        This will remove all the columns that contain the word Textbox in them. \n",
    "        This Function takes a DataFrame as in input and returns all the columns except TextBox. \n",
    "    \"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "    txt_chk = re.compile(r'Textbox')\n",
    "    tot_hrs_col_name = [\"RecordedHours2\",\"NonChargeHours2\",\"WOHours2\",\"TotalHour2\"]\n",
    "    new_cols = [col_name for col_name in cols if not(txt_chk.search(col_name)) and col_name not in tot_hrs_col_name]\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(dct, match):\n",
    "    \"\"\"\n",
    "        Takes a Dictionary and a filename as inputs and Returns how many rows need to be skipped for a filename. \n",
    "        Returns the Number of rows to skip, while creating a DataFrame.\n",
    "    \"\"\"\n",
    "    for val in dct.keys():\n",
    "        if re.match(val, match):\n",
    "            return dct[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_Filename(fname):\n",
    "    \"\"\"\n",
    "        Accepts a Filename that has fname_date.csv format. \n",
    "        It Extracts the From Date form the File and Returns the same. \n",
    "        These Are Datetime Objects.  \n",
    "        If the Filename has only start date, it will just return the same date for Both Start and End Date. \n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.compile(r'_\\d*')\n",
    "    match = pattern.findall(fname)\n",
    "    dt = match[0]\n",
    "    dt = dt.split(\"_\")[1]\n",
    "    file_date = pd.to_datetime(dt, format='%d%m%Y')\n",
    "    return file_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files(file_loc):\n",
    "    log_loc = file_loc + \"\\\\\" + \"Logs\"\n",
    "    cat_file_logger = lc.start_log(log_loc)\n",
    "    unprocessed = file_loc + \"\\\\Unprocessed\"\n",
    "    processed = file_loc + \"\\\\\" + \"Processed\"\n",
    "\n",
    "    if os.name == 'posix':\n",
    "        os.system('clear')\n",
    "    else:\n",
    "        os.system('cls')\n",
    "\n",
    "    process_files = {}\n",
    "    discard_files = {}\n",
    "    file_list = os.listdir(file_loc)\n",
    "\n",
    "    if os.path.exists(unprocessed):\n",
    "        cat_file_logger.info(f'\\n[Unprocessed] already Exists in [{file_loc}]\\n')\n",
    "    else:\n",
    "        os.mkdir(unprocessed)\n",
    "        cat_file_logger.info(f'\\nCreating Folder {unprocessed} in {file_loc}\\n')\n",
    "\n",
    "    # Segregating the Files.\n",
    "    discard_files['all_pie'] = [files for files in file_list if len(re.compile(r'[\\sa-zA-Z\\s]+Pie \\w+_\\d+.csv').findall(files))]\n",
    "    discard_files['all_xlsx'] = [files for files in file_list if files.endswith(\".xlsx\")]\n",
    "\n",
    "    # Move the above files to Unprocessed Folder before moving ahead\n",
    "    for f in discard_files.keys():\n",
    "        [shutil.move(file_loc + \"\\\\\" + file, unprocessed)  for file in discard_files[f]]\n",
    "\n",
    "    process_files['client_billing'] = [files for files in file_list if len(re.compile(r'Client [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['fee_brkdn_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Breakdown [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['fee_summ_dept_fe'] = [files for files in file_list if len(re.compile(r'Fee Summary [a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['fees_billed'] = [files for files in file_list if len(re.compile(r'Fees B[a-zA-Z\\s]+_\\d+.csv').findall(files))]\n",
    "    process_files['matter_src'] = [files for files in file_list if len(re.compile(r'Matter Source [a-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    process_files['matter_opened'] = [files for files in file_list if len(re.compile(r'Matters Open[\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    process_files['payment_rcv'] = [files for files in file_list if len(re.compile(r'Payment [\\sa-zA-Z\\s()]+_\\d+.csv').findall(files))]\n",
    "    process_files['tot_hrs_fe'] = [files for files in file_list if len(re.compile(r'([tT]otal[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "\n",
    "    for f in process_files.keys():\n",
    "        print(f'Moving category [{f}]')\n",
    "        [shutil.move(file_loc + \"\\\\\" + file, processed)  for file in process_files[f]]\n",
    "    \n",
    "\n",
    "    return process_files\n",
    "    #:TODO: \n",
    "    # Add these files individually to a database.\n",
    "    # Then concatenate these files and add them to staging database. \n",
    "    # No Need to move files in separate folders. Create a Dataframe in memory and perform operations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = f\"D:\\Learning+Offline\\DATA_Dump\"\n",
    "categorize_files(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(dict_list, file_loc, logfile_loc):\n",
    "    concat_logger = lc.start_log(logfile_loc)\n",
    "    df_all_files = {}\n",
    "    tot_processed = 0\n",
    "     # If a Filename is not in this Dictionary, then it will not be Considered. \n",
    "    # date = (datetime.now()).strftime(\"%m-%d-%y\")\n",
    "    \n",
    "    for file_cat in dict_list.keys():\n",
    "        df_final = pd.DataFrame()\n",
    "        concat_logger.info('*' * 50)\n",
    "        concat_logger.info(f'Processing Category {file_cat}')\n",
    "        for file in dict_list[file_cat]:\n",
    "            dict_fname = file.split(\"_\")[0]\n",
    "            dfc_file = pd.read_csv((file_loc + \"\\\\\" + file), skiprows=get_rows(skip_rows, file))\n",
    "            dfc_file = dfc_file[remove_cols(dfc_file)]\n",
    "            processing_date = get_date_from_Filename(file)\n",
    "            dfc_file[\"Date_Added\"] = processing_date\n",
    "            df_final = pd.concat([df_final, dfc_file], ignore_index=True)\n",
    "            df_final.fillna(0)\n",
    "            df_final = df_final.replace(re.compile(r'Â£'), \"\").replace(re.compile(r','), \"\").replace(re.compile(r'\\('),\"\").replace(re.compile(r'\\)'), \"\")\n",
    "\n",
    "            for cols in df_final.columns:\n",
    "                try:\n",
    "                    df_final[cols].astype(float)\n",
    "                except:\n",
    "                    continue\n",
    "                    # concat_logger.info(f'Skipping Column {cols}')\n",
    "                else:\n",
    "                    # concat_logger.info(f'Converting {cols} to float')\n",
    "                    df_final[cols] = df_final[cols].astype(float)\n",
    "\n",
    "        df_all_files[dict_fname] = df_final\n",
    "\n",
    "    for f in df_all_files.keys():\n",
    "        rows = df_all_files[f].shape[0]\n",
    "        concat_logger.info(f'Will Add -> {rows} entries for [{f}] to the database')\n",
    "\n",
    "    return df_all_files"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
