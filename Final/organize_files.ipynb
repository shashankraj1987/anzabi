{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil \n",
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_loc = \"/home/shashankraj/Documents/DATA/\"\n",
    "file_loc = r'C:\\Users\\shash\\OneDrive - Anza Services LLP\\DATA_Dump'\n",
    "unprocessed = file_loc+\"\\\\Unprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_rows = {}\n",
    "skip_rows['Client Billing Descending'] = 0\n",
    "skip_rows[\"Fee Breakdown by Dept and Fee Earner\"] = 3\n",
    "skip_rows[\"Fee Summary by Dept and Fee Earner\"] = 3\n",
    "skip_rows[\"Fees Billed\"] = 3\n",
    "skip_rows[\"Matter Source of Business inc Matter Bills\"] = 0\n",
    "skip_rows[\"Matters Opened by FE\"] = 3\n",
    "skip_rows[\"Payment Received Analysis\"] = 3\n",
    "skip_rows[\"Total Hours by Fee Earner-With Billings\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_csv_files(fold_path):\n",
    "    if os.path.exists(fold_path):\n",
    "        count_csv = 0\n",
    "        count_xlsx = 0\n",
    "        for root,dirs,files in os.walk(fold_path):\n",
    "            count_csv+= len([f for f in files if f.endswith('.csv')])\n",
    "            count_xlsx+= len([f for f in files if f.endswith('.xlsx')])\n",
    "        print(\"{} has {} CSV files \".format(fold_path,count_csv))\n",
    "        print(\"{} has {} XLSX files \".format(fold_path,count_xlsx))\n",
    "\n",
    "    for name in os.listdir(fold_path):\n",
    "        path = os.path.join(fold_path,name)\n",
    "        #print(path)\n",
    "        if os.path.isdir(path):\n",
    "            count_csv_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df):\n",
    "    \"\"\" \n",
    "    This will remove all the columns that contain the word Textbox in them. \n",
    "    This Function takes a DataFrame as in input and returns all the columns except TextBox. \n",
    "\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "\n",
    "    for x in cols:\n",
    "        txt_chk = re.compile(r'Textbox')\n",
    "        if txt_chk.search(x)== None:\n",
    "            new_cols.append(x)\n",
    "        else: \n",
    "            continue\n",
    "    \n",
    "    return(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(dictnry, match):\n",
    "    \"\"\"\n",
    "        Takes a Dictionary and a filename as inputs and Returns how many rows need to be skipped for a filename. \n",
    "        Returns the Number of rows to skip, while creating a DataFrame.\n",
    "    \"\"\"\n",
    "    for val in dictnry.keys():\n",
    "        if re.match(val,match):\n",
    "            return (dictnry[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_Filename(fname):\n",
    "    \"\"\"\n",
    "        Accepts a Filename that has fname_date.csv format. \n",
    "        It Extracts the From Date form the File and Returns the same. \n",
    "        These Are Datetime Objects.  \n",
    "\n",
    "        If the Filename has only start date, it will just return the same date for Both Start and End Date. \n",
    "\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'_\\d*')\n",
    "    match = pattern.findall(fname)\n",
    "    dt = match[0]\n",
    "    dt = dt.split(\"_\")[1]\n",
    "    file_date = pd.to_datetime(dt,format='%d%m%Y')\n",
    "    \n",
    "    return file_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_file_names(files):\n",
    "    \"\"\" \n",
    "        This Function will Check a Given Location for all files and find the Unique File Names. \n",
    "        It Splits on the \"_\" as that is the current Naming Convention.\n",
    "        Also, it only find .csv files\n",
    "        Returns the Unique File Names.\n",
    "    \"\"\"\n",
    "    all_csvs = []\n",
    "    all_file_names = []\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            all_csvs.append(file)\n",
    "\n",
    "    for file in all_csvs:\n",
    "        fname = file.split(\"_\")[0]\n",
    "        all_file_names.append(fname)\n",
    "\n",
    "    all_file_names =  np.unique(all_file_names)\n",
    "    return all_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files(loc):\n",
    "    if os.name == 'posix':\n",
    "        os.system('clear')\n",
    "    else:\n",
    "        os.system('cls')\n",
    "    \n",
    "    process_files = {}\n",
    "    discard_files = {}\n",
    "    i=0\n",
    "    j=0\n",
    "    total = 0   \n",
    "    file_list = os.listdir(loc)\n",
    "\n",
    "    if os.path.exists(unprocessed):\n",
    "        print(f'\\n[Unprocessed] already Exists in [{file_loc}]\\n')\n",
    "    else: \n",
    "        os.mkdir(unprocessed)\n",
    "        print(f'\\nCreating Folder {unprocessed} in {file_loc}\\n')\n",
    "\n",
    "    ## Seggregating the Files.\n",
    "    discard_files['all_pie'] = [files for files in file_list if len(re.compile(r'[\\sa-zA-Z\\s]+Pie \\w+_\\d+.csv').findall(files))]\n",
    "    discard_files['all_xlsx'] = [files for files in file_list if files.endswith(\".xlsx\")]    \n",
    "\n",
    "    # Move the above files to Unprocessed Folder before moving ahead\n",
    "    for i in discard_files.keys():\n",
    "        for j in discard_files[i]:\n",
    "            shutil.move(file_loc+\"\\\\\"+j,unprocessed)\n",
    "            print(f'Moving File -- {j} to [{unprocessed}]')\n",
    "            total+=1\n",
    "\n",
    "    print(f'\\nMoved total {total} files to Unprocessed Folder')\n",
    "\n",
    "    process_files['all_client'] = [files for files in file_list if len(re.compile(r'([cC]lient[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "    process_files['all_fees'] = [files for files in file_list if len(re.compile(r'([fF]ee[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files)) & files.startswith('Fee')]\n",
    "    process_files['all_matter'] = [files for files in file_list if len(re.compile(r'([mM]atter[\\sa-z-A-Z\\s]*\\(Bill Date\\)_\\d+.csv)').findall(files))]\n",
    "    process_files['all_payments'] = [files for files in file_list if len(re.compile(r'([pP]ayment[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "    process_files['all_total'] = [files for files in file_list if len(re.compile(r'([tT]otal[\\sa-z-A-Z\\s]*_\\d+.csv)').findall(files))]\n",
    "    \n",
    "    return process_files, total\n",
    "    #:TODO: \n",
    "    # Add these files individually to a database.\n",
    "    # Then concatenate these files and add them to staging database. \n",
    "    # No Need to move files in separate folders. Create a Dataframe in memory and perform operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(dict_list):\n",
    "    df_final = pd.DataFrame()\n",
    "    # If a Filename is not in this Dictionary, then it will not be Considered. \n",
    "    date = (datetime.now()).strftime(\"%m-%d-%y\")\n",
    "    for file_cat in dict_list.keys():\n",
    "        print(f'\\nProcessing Category {file_cat}')\n",
    "        print(\"*\"*50)\n",
    "        for file in dict_list[file_cat]:\n",
    "            #dfc_file = pd.read_csv((file_loc+file), skiprows=get_rows(skip_rows,file))\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Unprocessed] already Exists in [C:\\Users\\shash\\OneDrive - Anza Services LLP\\DATA_Dump]\n",
      "\n",
      "\n",
      "Moved total 0 files to Unprocessed Folder\n",
      "\n",
      "Processing Category all_client\n",
      "**************************************************\n",
      "Client Billing Descending_01032022.csv\n",
      "Client Billing Descending_02032022.csv\n",
      "Client Billing Descending_03032022.csv\n",
      "Client Billing Descending_04032022.csv\n",
      "Client Billing Descending_07032022.csv\n",
      "Client Billing Descending_08032022.csv\n",
      "Client Billing Descending_09032022.csv\n",
      "Client Billing Descending_10032022.csv\n",
      "Client Billing Descending_11032022.csv\n",
      "\n",
      "Processing Category all_fees\n",
      "**************************************************\n",
      "Fee Breakdown by Dept and Fee Earner_01032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_02032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_03032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_04032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_07032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_08032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_09032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_10032022.csv\n",
      "Fee Breakdown by Dept and Fee Earner_11032022.csv\n",
      "Fee Summary by Dept and Fee Earner_01032022.csv\n",
      "Fee Summary by Dept and Fee Earner_02032022.csv\n",
      "Fee Summary by Dept and Fee Earner_03032022.csv\n",
      "Fee Summary by Dept and Fee Earner_04032022.csv\n",
      "Fee Summary by Dept and Fee Earner_07032022.csv\n",
      "Fee Summary by Dept and Fee Earner_08032022.csv\n",
      "Fee Summary by Dept and Fee Earner_09032022.csv\n",
      "Fee Summary by Dept and Fee Earner_10032022.csv\n",
      "Fee Summary by Dept and Fee Earner_11032022.csv\n",
      "Fees Billed_01032022.csv\n",
      "Fees Billed_02032022.csv\n",
      "Fees Billed_03032022.csv\n",
      "Fees Billed_04032022.csv\n",
      "Fees Billed_07032022.csv\n",
      "Fees Billed_08032022.csv\n",
      "Fees Billed_09032022.csv\n",
      "Fees Billed_10032022.csv\n",
      "Fees Billed_11032022.csv\n",
      "\n",
      "Processing Category all_matter\n",
      "**************************************************\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_01032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_02032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_03032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_04032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_07032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_08032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_09032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_10032022.csv\n",
      "Matter Source of Business inc Matter Bills (Bill Date)_11032022.csv\n",
      "\n",
      "Processing Category all_payments\n",
      "**************************************************\n",
      "Payment Received Analysis_01032022.csv\n",
      "Payment Received Analysis_02032022.csv\n",
      "Payment Received Analysis_03032022.csv\n",
      "Payment Received Analysis_04032022.csv\n",
      "Payment Received Analysis_07032022.csv\n",
      "Payment Received Analysis_08032022.csv\n",
      "Payment Received Analysis_09032022.csv\n",
      "Payment Received Analysis_10032022.csv\n",
      "Payment Received Analysis_11032022.csv\n",
      "\n",
      "Processing Category all_total\n",
      "**************************************************\n",
      "Total Hours by Fee Earner-With Billings All_01032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_02032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_03032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_04032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_05032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_06032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_07032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_08032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_09032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_10032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_11032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_12032022.csv\n",
      "Total Hours by Fee Earner-With Billings All_13032022.csv\n"
     ]
    }
   ],
   "source": [
    "all_file_loc = categorize_files(file_loc)[0]\n",
    "concat_files(all_file_loc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
