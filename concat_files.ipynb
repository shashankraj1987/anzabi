{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheets Usage Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client Billing Descending -- Entire Sheet Not Used \n",
    "\n",
    "Fees By Worktype PIE Chart -- Entire Sheet not used  -- No Need to Process This \n",
    "\n",
    "Fees Summary by Dept and Fee Earner -- Entire Sheet Not Used\n",
    "\n",
    "============================================================\n",
    "\n",
    "Fee Breakdown by Dept and Fee earner -- Yes Except Textbox columns \n",
    "\n",
    "Fees Billed -- Yes Except TextBox Columns \n",
    "\n",
    "Matters Opened by FE -- Yes Except TextBox Columns \n",
    "\n",
    "Matter Sourece of Business Inc Matter Bills - Yes Except TextBox columns \n",
    "\n",
    "Payment Received Analysis  - Yes Except TextBox columns (Both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_rows = {}\n",
    "\n",
    "skip_rows[\"Fee Breakdown by Dept and Fee Earner\"] = 3\n",
    "skip_rows[\"Fees Billed\"] = 3\n",
    "skip_rows[\"Matters Opened by FE\"] = 3\n",
    "skip_rows[\"Matter Source of Business inc Matter Bills\"] = 0\n",
    "skip_rows[\"Payment Received Analysis\"] = 3\n",
    "skip_rows[\"Total Hours by Fee Earner-With Billings\"] = 0\n",
    "\n",
    "# skip_rows[\"Fees by Worktype Pie Chart\"] = 3\n",
    "# skip_rows[\"Fee Summary by Dept and Fee Earner\"] = 3\n",
    "# skip_rows[\"Client Billing Descending\"] = 0\n",
    "\n",
    "# If a Filename is not in this Dictionary, then it will not be Considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df):\n",
    "    \"\"\" \n",
    "    This will remove all the columns that contain the word Textbox in them. \n",
    "    This Function takes a DataFrame as in input and returns all the columns except TextBox. \n",
    "\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    new_cols = []\n",
    "\n",
    "    for x in cols:\n",
    "        txt_chk = re.compile(r'Textbox')\n",
    "        if txt_chk.search(x)== None:\n",
    "            new_cols.append(x)\n",
    "        else: \n",
    "            continue\n",
    "    \n",
    "    return(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(loc, file_name,file_names_array, final_files_loc):\n",
    "    \"\"\"\n",
    "    loc = where these files are \n",
    "    file_name = current file name being processed \n",
    "    files names array = All the files that need to be concatenated \n",
    "\n",
    "    \"\"\"\n",
    "    df_final = pd.DataFrame()\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%m-%d-%y\") \n",
    "    print(f'Processing {file_name}')\n",
    "\n",
    "    for file in file_names_array:\n",
    "        dfc_file=pd.read_csv((loc+file), skiprows=get_rows(skip_rows,file))\n",
    "        dfc_file=dfc_file[remove_cols(dfc_file)]\n",
    "        \n",
    "        st_end_dates = get_start_enddate_from_Filename(file)\n",
    "        dfc_file[\"From_Date\"] = st_end_dates[0]\n",
    "        print(f'End Date from file is {st_end_dates[1]}')\n",
    "        dfc_file[\"To_Date\"] = st_end_dates[1]\n",
    "        df_final = pd.concat([df_final,dfc_file],ignore_index=True)\n",
    "\n",
    "    # return df_final\n",
    "    final_file = final_files_loc+file_name+\"_\"+date+\"_.csv\"\n",
    "    df_final.to_csv(final_file,index=False,)\n",
    "    print(f'Created {final_file}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_enddate_from_Filename(fname):\n",
    "    \"\"\"\n",
    "        Accepts a Filename that has fname_date.csv format or fname_datetodate.csv format. \n",
    "        It Extracts the From and To Date form the File and Returns 2 Objects \n",
    "        These Are Datetime Objects. First is Start Date and Second is End Date. \n",
    "\n",
    "        If the Filename has only start date, it will just return the same date for Both Start and End Date. \n",
    "\n",
    "    \"\"\"\n",
    "    # suffx=fname.split(\"_\")[1]\n",
    "    # print(f'Suffix is {suffx}')\n",
    "    # if len(suffx)  > 12:\n",
    "    #     print(\"\\nIgnoring Monthly Files\")\n",
    "    #     # pattern = re.compile(r'_\\d*\\w*\\d')\n",
    "        # match = pattern.findall(fname)\n",
    "        # dt = match[0]\n",
    "        # dt1 = dt.split(\"_\")[1].split(\"to\")[0]\n",
    "        # dt2 = dt.split(\"_\")[1].split(\"to\")[1]\n",
    "        # start_date = pd.to_datetime(dt1,format='%d%m%Y')\n",
    "        # end_date = pd.to_datetime(dt2,format='%d%m%Y')\n",
    "    # else:\n",
    "    #  \n",
    "    \n",
    "    pattern = re.compile(r'_\\d*')\n",
    "    match = pattern.findall(fname)\n",
    "    dt = match[0]\n",
    "    dt = dt.split(\"_\")[1]\n",
    "    start_date = pd.to_datetime(dt,format='%d%m%Y')\n",
    "    end_date = start_date\n",
    "    \n",
    "    return start_date,end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(dictnry, match):\n",
    "    \"\"\"\n",
    "        Takes a Dictionary and a filename as inputs and Returns how many rows need to be skipped for a filename. \n",
    "        Returns the Number of rows to skip, while creating a DataFrame.\n",
    "    \"\"\"\n",
    "    for val in skip_rows.keys():\n",
    "        if re.match(val,match):\n",
    "            return (skip_rows[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_file_names(files):\n",
    "\n",
    "    \"\"\" \n",
    "        This Function will Check a Given Location for all files and find the Unique File Names. \n",
    "        It Splits on the \"_\" as that is the current Naming Convention.\n",
    "        Also, it only find .csv files\n",
    "        Returns the Unique File Names.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_csvs = []\n",
    "    all_file_names = []\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            all_csvs.append(file)\n",
    "\n",
    "    for file in all_csvs:\n",
    "        fname = file.split(\"_\")[0]\n",
    "        all_file_names.append(fname)\n",
    "\n",
    "    all_file_names =  np.unique(all_file_names)\n",
    "    return all_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_loc = \"/home/shashankraj/Documents/One_Drv_Anza/test/\"\n",
    "final_xl_files_loc = \"/home/shashankraj/Documents/One_Drv_Anza/Final_Files/\"\n",
    "data_files_list = os.listdir(data_file_loc)\n",
    "data_files_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_processing(files_loc,files_list,final_files_loc):\n",
    "    print(\"Inside Final Processing\")\n",
    "    unique_files = get_unique_file_names(files_list) ## I need output of os.listdir\n",
    "    for each_file in unique_files:\n",
    "        print(f'\\nChecking [{each_file}]')\n",
    "        if each_file == \"Matter Source of Business inc Matter Bills (Bill Date)\":\n",
    "            each_file = \"Matter Source of Business inc Matter Bills\"\n",
    "        if each_file in skip_rows.keys():\n",
    "            pattern = re.compile(each_file)\n",
    "            each_file_arr = [file for file in files_list if len(pattern.findall(file))]\n",
    "            print(each_file_arr)\n",
    "            concat_files(files_loc,each_file,each_file_arr,final_files_loc)  \n",
    "        else: \n",
    "            print(f'Skipping Processing of [{each_file}]')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_processing(data_file_loc, data_files_list,final_xl_files_loc)\n",
    "\n",
    "\n",
    "#TODO: Find a Way to Break Each File Operation into a Single Thread. Then Use Multithreading or Multiprocessing. \n",
    "# TODO: This will be a One Time Operation at a given time. Is it worth the Time Spent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc = \"/home/shashankraj/Documents/One_Drv_Anza/test/\"\n",
    "# file_name = \"Fee Breakdown by Dept and Fee Earner\"\n",
    "# file_names_array = ['Fee Breakdown by Dept and Fee Earner_11012022.csv', 'Fee Breakdown by Dept and Fee Earner_20122021.csv', \n",
    "# 'Fee Breakdown by Dept and Fee Earner_18112021.csv']\n",
    "\n",
    "# final  = concat_files(loc,file_name,file_names_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_loc = \"/home/shashankraj/Documents/One_Drv_Anza/test/\"\n",
    "\n",
    "# c_file = os.listdir(file_loc)\n",
    "# c_file = c_file[5]\n",
    "# c_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = pd.DataFrame()\n",
    "# dfc_file = pd.read_csv((file_loc+c_file), skiprows=get_rows(skip_rows,c_file))\n",
    "# dfc_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfc_file = dfc_file[remove_cols(dfc_file)]\n",
    "# dfc_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_end_dates = get_start_enddate_from_Filename(c_file)\n",
    "# dfc_file[\"From_Date\"] = st_end_dates[0]\n",
    "# print(f'Start Date from file is {st_end_dates[0]}')\n",
    "# dfc_file[\"To_Date\"] = st_end_dates[1]\n",
    "# dfc_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location = \"/home/shashankraj/Documents/One_Drv_Anza/test\"\n",
    "# curr_file = get_unique_file_names(os.listdir(location))[5]\n",
    "# curr_file\n",
    "\n",
    "# pattern = re.compile(r'Matter Source of Business inc Matter Bills')\n",
    "# x = [file for file in os.listdir(location) if len(pattern.findall(file))]\n",
    "\n",
    "# print(x)\n",
    "\n",
    "# print(\"*\"*50)\n",
    "\n",
    "# print(len(x))\n",
    "\n",
    "# concat_files(location,curr_file,x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efeb6664015e98725bbb5b8b87814de131c81a1fd666b7e6d11b4ba6e020f0b0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
